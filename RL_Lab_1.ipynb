{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Lab: Bandits\n",
        "\n",
        "In this lab, you take on the role of a data scientist at **NewsNet**, a company that personalizes news recommendations. Every time a user logs in, you observe their profile (**context**) and recommend one of several articles. The user may click (**reward = 1**) or ignore (**reward = 0**).\n",
        "\n",
        "You must learn to make better decisions over time by balancing **exploration** and **exploitation**.  \n",
        "This is modeled using **bandit algorithms**.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Multi-Armed Bandits\n",
        "In this part, you will start with the classical **multi-armed bandit** problem, where each article has a fixed but unknown probability of being clicked. Your goal is to learn which article performs best through trial and error.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2: Contextual Bandits\n",
        "In the second part, you will extend the problem to **contextual bandits**, where the user’s profile (context) affects which article is most relevant. You will use the context to make more personalized and effective recommendations.\n"
      ],
      "metadata": {
        "id": "p2GuQTxpPHhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Multi-Armed Bandits\n",
        "\n",
        "We use the following **environment** to simulate the news recommendation scenario.  \n",
        "Each article (or *arm*) has an unknown probability of being clicked. When you recommend an article (choose an arm), the environment returns a reward of `1` (user clicks) or `0` (user ignores).  \n",
        "\n",
        "Your goal is to learn which article performs best through interaction with this environment.\n"
      ],
      "metadata": {
        "id": "H9DVxcgtPXZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL2FIgmoPBRd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class BernoulliBandit:\n",
        "    \"\"\"\n",
        "    A simple environment for the multi-armed bandit problem.\n",
        "    Each arm (article) has a fixed probability of giving a reward (user click).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probs):\n",
        "        \"\"\"\n",
        "        Initialize the bandit with a list of probabilities for each arm.\n",
        "\n",
        "        Input:\n",
        "            probs (list of float): Probabilities of reward for each arm.\n",
        "        \"\"\"\n",
        "        self.probs = probs\n",
        "        self.k = len(probs)\n",
        "\n",
        "    def pull(self, arm):\n",
        "        \"\"\"\n",
        "        Simulate pulling an arm (recommending an article).\n",
        "\n",
        "        Input:\n",
        "            arm (int): Index of the arm/article to pull.\n",
        "\n",
        "        Output:\n",
        "            reward (int): 1 if user clicks, 0 otherwise.\n",
        "        \"\"\"\n",
        "        return int(np.random.rand() < self.probs[arm])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 (a): Epsilon-Greedy\n",
        "\n",
        "Now implement **epsilon-greedy** to learn the best arm. Follow the lab instructions.  \n",
        "Below we provide starter code that can be used—**for the algorithm it is enough to change the marked text** (look for lines marked `TODO`).\n"
      ],
      "metadata": {
        "id": "-rfY34I7SGGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-Greedy — self-contained run function + quick test ========\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_epsilon_greedy(probs, T, epsilon=0.1, seed=0):\n",
        "    \"\"\"\n",
        "    Run epsilon-greedy for T steps on a BernoulliBandit with given probs.\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "          'rewards': np.array[T],\n",
        "          'regrets': np.array[T],\n",
        "          'actions': np.array[T],\n",
        "          'Q': np.array[k],   # final value estimates\n",
        "          'N': np.array[k],   # pull counts\n",
        "        }\n",
        "    \"\"\"\n",
        "    assert all(0.0 <= p <= 1.0 for p in probs), \"All probabilities must be in [0, 1]\"\n",
        "    bandit = BernoulliBandit(probs)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    k = len(probs)\n",
        "    Q = np.zeros(k, dtype=float)   # value estimates\n",
        "    N = np.zeros(k, dtype=int)     # pull counts\n",
        "\n",
        "    rewards = np.zeros(T, dtype=int)\n",
        "    regrets = np.zeros(T, dtype=float)\n",
        "    actions = np.zeros(T, dtype=int)\n",
        "\n",
        "    optimal_mean = float(np.max(probs))\n",
        "\n",
        "    for t in range(1, T + 1):\n",
        "        # ----------------------- TODO: ε-greedy selection ----------------------\n",
        "        # With probability epsilon: choose a random arm.\n",
        "        # Otherwise: choose the arm with the highest Q (break ties via np.argmax).\n",
        "        #\n",
        "        # Hint: rng.random() for the coin flip; rng.integers(0, k) for random arm.\n",
        "        # Hint: np.argmax(Q) for exploitation.\n",
        "\n",
        "        arm = rng.integers(0, k)  # TEMP PLACEHOLDER — replace with ε-greedy\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        r = int(bandit.pull(arm))\n",
        "        rewards[t - 1] = r\n",
        "        actions[t - 1] = arm\n",
        "        regrets[t - 1] = optimal_mean - probs[arm]\n",
        "\n",
        "        N[arm] += 1\n",
        "        Q[arm] += (r - Q[arm]) / N[arm]\n",
        "\n",
        "    return {\"rewards\": rewards, \"regrets\": regrets, \"actions\": actions, \"Q\": Q, \"N\": N}\n",
        "\n",
        "# --- Quick single-run test / visualization for ε-greedy ---\n",
        "\n",
        "##############################################################\n",
        "###\n",
        "### You can play around with these parameters:\n",
        "###\n",
        "probs = [0.2, 0.1, 0.5, 0.9]\n",
        "T = 1000\n",
        "epsilon = 0.1\n",
        "seed = 123\n",
        "##############################################################\n",
        "\n",
        "out_eps = run_epsilon_greedy(probs, T, epsilon, seed)\n",
        "\n",
        "plt.figure(figsize=(12, 3.5))\n",
        "plt.subplot(1, 3, 1)\n",
        "avg_reward = np.cumsum(out_eps[\"rewards\"]) / np.arange(1, T + 1)\n",
        "plt.plot(avg_reward); plt.title(\"ε-Greedy: Avgerage Reward\"); plt.xlabel(\"t\"); plt.ylabel(\"avg\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(np.cumsum(out_eps[\"regrets\"])); plt.title(\"ε-Greedy: Cumulative Regret\"); plt.xlabel(\"t\"); plt.ylabel(\"regret\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "counts = np.bincount(out_eps[\"actions\"], minlength=len(probs))\n",
        "plt.bar(range(len(probs)), counts); plt.title(\"ε-Greedy: Pull Counts\"); plt.xlabel(\"arm\"); plt.ylabel(\"# pulls\")\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "bOdYchea7ziK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 (b): UCB\n",
        "\n",
        "Now implement **UCB** (Upper Confidence Bound) to learn the best arm. Follow the lab instructions.  \n",
        "Below we provide starter code that can be used—**for the algorithm it is enough to change the marked text** (look for lines marked `TODO`).\n"
      ],
      "metadata": {
        "id": "yUrk-TAFBSrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UCB — self-contained run function + quick test ===================\n",
        "\n",
        "def run_ucb(probs, T, c=1.0, seed=0):\n",
        "    \"\"\"\n",
        "    Run Upper Confidence Bound (UCB) for T steps on a BernoulliBandit.\n",
        "\n",
        "    Returns:\n",
        "        dict: same structure as run_epsilon_greedy\n",
        "    \"\"\"\n",
        "    assert all(0.0 <= p <= 1.0 for p in probs), \"All probabilities must be in [0, 1]\"\n",
        "    bandit = BernoulliBandit(probs)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    k = len(probs)\n",
        "    Q = np.zeros(k, dtype=float)\n",
        "    N = np.zeros(k, dtype=int)\n",
        "\n",
        "    rewards = np.zeros(T, dtype=int)\n",
        "    regrets = np.zeros(T, dtype=float)\n",
        "    actions = np.zeros(T, dtype=int)\n",
        "\n",
        "    optimal_mean = float(np.max(probs))\n",
        "\n",
        "    for t in range(1, T + 1):\n",
        "        # --------------------------- TODO: UCB selection -----------------------\n",
        "        # Compute: UCB[a] = Q[a] + c * sqrt( (ln(t)) / N[a] )\n",
        "        # If N[a] == 0, set UCB[a] = +inf so each arm is tried at least once.\n",
        "        # Then choose the arm with the largest UCB[a].\n",
        "        #\n",
        "        # Hints:\n",
        "        #   - Use np.inf for infinity, np.log(t) for log.\n",
        "        #   - Create an array of UCB values of length k.\n",
        "\n",
        "        arm = rng.integers(0, k)  # TEMP PLACEHOLDER — replace with UCB rule\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        r = int(bandit.pull(arm))\n",
        "        rewards[t - 1] = r\n",
        "        actions[t - 1] = arm\n",
        "        regrets[t - 1] = optimal_mean - probs[arm]\n",
        "\n",
        "        N[arm] += 1\n",
        "        Q[arm] += (r - Q[arm]) / N[arm]\n",
        "\n",
        "    return {\"rewards\": rewards, \"regrets\": regrets, \"actions\": actions, \"Q\": Q, \"N\": N}\n",
        "\n",
        "# --- Quick single-run test / visualization for UCB ---\n",
        "\n",
        "##############################################################\n",
        "###\n",
        "### You can play around with these parameters:\n",
        "###\n",
        "probs = [0.2, 0.1, 0.5, 0.9]\n",
        "T = 1000\n",
        "c=1.0\n",
        "seed=123\n",
        "##############################################################\n",
        "\n",
        "\n",
        "out_ucb = run_ucb(probs, T, c, seed)\n",
        "\n",
        "plt.figure(figsize=(12, 3.5))\n",
        "plt.subplot(1, 3, 1)\n",
        "avg_reward = np.cumsum(out_ucb[\"rewards\"]) / np.arange(1, T + 1)\n",
        "plt.plot(avg_reward); plt.title(\"UCB: Avgerage Reward\"); plt.xlabel(\"t\"); plt.ylabel(\"avg\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(np.cumsum(out_ucb[\"regrets\"])); plt.title(\"UCB: Cumulative Regret\"); plt.xlabel(\"t\"); plt.ylabel(\"regret\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "counts = np.bincount(out_ucb[\"actions\"], minlength=len(probs))\n",
        "plt.bar(range(len(probs)), counts); plt.title(\"UCB: Pull Counts\"); plt.xlabel(\"arm\"); plt.ylabel(\"# pulls\")\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "KOMpooQ87znl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Contextual Bandits"
      ],
      "metadata": {
        "id": "lNC0mo6iK-EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Contextual Bandits\n",
        "\n",
        "This part extends the bandit problem to include **contextual information** such as user preferences and age.\n",
        "\n",
        "Below are three code cells corresponding to this part:\n",
        "\n",
        "1. **Environment and Policies** – sets up the contextual bandit environment and the algorithms described in the lab instructions (Random, Contextual ε-Greedy, and LinUCB).  \n",
        "2. **Long-Run Simulation** – runs all three algorithms and plots the **Average Reward** and **Cumulative Regret** over time, showing how each learns to recommend better articles.  \n",
        "3. **Illustrative Simulation** – shows detailed examples of the first 6 interactions and, after 10,000 learning steps, how the recommendations have improved based on what each algorithm has learned.\n"
      ],
      "metadata": {
        "id": "660NyGm2pANZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Environment and Policies =================================\n",
        "import numpy as np\n",
        "\n",
        "class ContextualNewsEnv:\n",
        "    \"\"\"\n",
        "    Contextual bandit for news with interpretable features, incl. age.\n",
        "    Reward ~ Bernoulli(sigmoid(theta[a] · x)).\n",
        "    \"\"\"\n",
        "    def __init__(self, seed=0):\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        # 6 human-readable features (last one is age_z = (age - 40)/15)\n",
        "        self.feature_names = [\n",
        "            \"likes_politics\",\n",
        "            \"sports_fan\",\n",
        "            \"techie\",\n",
        "            \"mobile_user\",\n",
        "            \"morning_reader\",\n",
        "            \"age_z\",             # standardized age (0 ≈ 40y, +1 ≈ 55y, -1 ≈ 25y)\n",
        "        ]\n",
        "        self.d = len(self.feature_names)\n",
        "        self.arm_names = [\"Politics\", \"Sports\", \"Tech\", \"Lifestyle\"]\n",
        "        self.k = len(self.arm_names)\n",
        "\n",
        "        # True arm parameters (rows=arms, cols=features); include age effects\n",
        "        self.theta = np.array([\n",
        "            [ 1.6,  0.2,  0.1,  0.2,  0.7,  0.4],   # Politics prefers older & morning readers\n",
        "            [ 0.1,  1.8,  0.1,  0.7,  0.2, -0.1],   # Sports slightly skew younger/mobile\n",
        "            [ 0.0,  0.1,  1.9, -0.1, -0.2, -0.2],   # Tech slightly skew younger\n",
        "            [ 0.3,  0.2,  0.2,  1.0,  0.8,  0.0],   # Lifestyle mostly device/time driven\n",
        "        ], dtype=float)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(z):\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def _sample_age_z(self):\n",
        "        \"\"\"\n",
        "        Sample an age in years, then standardize:\n",
        "            age_z = (age - 40) / 15\n",
        "        (≈ N(40, 12^2) clipped to [18, 80] for realism)\n",
        "        \"\"\"\n",
        "        age = float(np.clip(self.rng.normal(40, 12), 18, 80))\n",
        "        return (age - 40.0) / 15.0\n",
        "\n",
        "    def sample_context(self):\n",
        "        \"\"\"\n",
        "        Sample an interpretable user:\n",
        "        pick a coarse segment, then add noise; append standardized age.\n",
        "        \"\"\"\n",
        "        seg = self.rng.choice([\"politics\", \"sports\", \"tech\", \"on_the_go\", \"morning_person\"])\n",
        "        x = np.zeros(self.d)\n",
        "        if seg == \"politics\":\n",
        "            core = np.array([1.6, 0.2, 0.2, 0.3, 0.9])\n",
        "        elif seg == \"sports\":\n",
        "            core = np.array([0.2, 1.8, 0.2, 1.0, 0.3])\n",
        "        elif seg == \"tech\":\n",
        "            core = np.array([0.2, 0.2, 1.9, 0.3, 0.2])\n",
        "        elif seg == \"on_the_go\":\n",
        "            core = np.array([0.4, 0.9, 0.5, 1.8, 0.7])\n",
        "        else:  # morning_person\n",
        "            core = np.array([0.8, 0.3, 0.2, 0.6, 1.9])\n",
        "        core = core + self.rng.normal(0, 0.2, size=5)\n",
        "        age_z = self._sample_age_z()\n",
        "        x[:5] = core\n",
        "        x[5] = age_z\n",
        "        return x\n",
        "\n",
        "    def click_prob(self, arm, x):\n",
        "        return float(self._sigmoid(self.theta[arm] @ x))\n",
        "\n",
        "    def click(self, arm, x):\n",
        "        \"\"\"Return (reward, true_click_prob).\"\"\"\n",
        "        p = self.click_prob(arm, x)\n",
        "        r = int(self.rng.random() < p)\n",
        "        return r, p\n",
        "\n",
        "\n",
        "# ---- Policies ---------------------------------------------------------------\n",
        "class Policy:\n",
        "    def select(self, x: np.ndarray) -> int: raise NotImplementedError\n",
        "    def update(self, arm: int, x: np.ndarray, r: int): pass\n",
        "\n",
        "class RandomPolicy(Policy):\n",
        "    def __init__(self, k, seed=0):\n",
        "        self.k = k\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "    def select(self, x): return int(self.rng.integers(0, self.k))\n",
        "\n",
        "class ContextualEpsGreedy(Policy):\n",
        "    \"\"\"Linear model per arm; ε-greedy on θ̂_a · x with random tie-breaking.\"\"\"\n",
        "    def __init__(self, k, d, epsilon=0.1, seed=0):\n",
        "        self.eps = float(epsilon)\n",
        "        self.Q = np.zeros((k, d))\n",
        "        self.N = np.zeros(k, dtype=int)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "    def select(self, x):\n",
        "        if self.rng.random() < self.eps:\n",
        "            return int(self.rng.integers(0, self.Q.shape[0]))\n",
        "        scores = self.Q @ x\n",
        "        best = np.flatnonzero(scores == np.max(scores))\n",
        "        return int(self.rng.choice(best))\n",
        "    def update(self, arm, x, r):\n",
        "        self.N[arm] += 1\n",
        "        pred = self.Q[arm] @ x\n",
        "        self.Q[arm] += (r - pred) * x / self.N[arm]\n",
        "\n",
        "class LinUCB(Policy):\n",
        "    \"\"\"Per-arm ridge regression + UCB: θ̂_a^T x + α sqrt(x^T A_a^{-1} x).\"\"\"\n",
        "    def __init__(self, k, d, alpha=1.0, lambda_=1.0):\n",
        "        self.k, self.d = k, d\n",
        "        self.alpha = float(alpha)\n",
        "        self.A = [lambda_ * np.eye(d) for _ in range(k)]\n",
        "        self.b = [np.zeros(d) for _ in range(k)]\n",
        "    def select(self, x):\n",
        "        vals = np.empty(self.k)\n",
        "        for a in range(self.k):\n",
        "            A_inv_x = np.linalg.solve(self.A[a], x)\n",
        "            theta_hat = np.linalg.solve(self.A[a], self.b[a])\n",
        "            pred = theta_hat @ x\n",
        "            bonus = self.alpha * np.sqrt(x @ A_inv_x)\n",
        "            vals[a] = pred + bonus\n",
        "        return int(np.argmax(vals))\n",
        "    def update(self, arm, x, r):\n",
        "        self.A[arm] += np.outer(x, x)\n",
        "        self.b[arm] += r * x\n"
      ],
      "metadata": {
        "id": "39O6-eKM7z88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Long-run simulation with Average Reward & Cumulative Regret =====\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Parameters you can play with -------------------------------------------\n",
        "T = 20000         # horizon (longer run for clearer curves)\n",
        "epsilon = 0.1     # contextual ε-greedy exploration rate\n",
        "alpha = 1.0       # LinUCB exploration bonus\n",
        "lambda_ = 1.0     # LinUCB regularization\n",
        "seed = 7          # master seed for reproducibility\n",
        "\n",
        "# ---- Generate one fixed context sequence (fair comparison across policies) ---\n",
        "env_for_contexts = ContextualNewsEnv(seed=seed)\n",
        "contexts = [env_for_contexts.sample_context() for _ in range(T)]\n",
        "\n",
        "# ---- Helper to run a policy on the fixed context sequence -------------------\n",
        "def run_on_contexts(policy, env_seed, contexts):\n",
        "    env = ContextualNewsEnv(seed=env_seed)  # fresh env so θ and RNG are consistent\n",
        "    rewards = np.zeros(len(contexts), dtype=int)\n",
        "    inst_regret = np.zeros(len(contexts), dtype=float)\n",
        "    for t, x in enumerate(contexts):\n",
        "        arm = policy.select(x)\n",
        "        r, p_sel = env.click(arm, x)\n",
        "        rewards[t] = r\n",
        "        # compute instantaneous regret in *expected* terms:\n",
        "        # p*(x) - p_sel(x), where p*(x)=max_a sigmoid(θ_a · x)\n",
        "        true_scores = env.theta @ x\n",
        "        p_star = 1.0 / (1.0 + np.exp(-np.max(true_scores)))\n",
        "        inst_regret[t] = p_star - p_sel\n",
        "        policy.update(arm, x, r)\n",
        "    return rewards, inst_regret\n",
        "\n",
        "# ---- Run Random, Contextual ε-Greedy, LinUCB --------------------------------\n",
        "rng = np.random.default_rng(seed)\n",
        "k = ContextualNewsEnv(seed=seed).k\n",
        "d = ContextualNewsEnv(seed=seed).d\n",
        "\n",
        "rewards_rand, regret_rand = run_on_contexts(RandomPolicy(k, seed=seed), seed, contexts)\n",
        "rewards_eps,  regret_eps  = run_on_contexts(ContextualEpsGreedy(k, d, epsilon, seed), seed, contexts)\n",
        "rewards_ucb,  regret_ucb  = run_on_contexts(LinUCB(k, d, alpha=alpha, lambda_=lambda_), seed, contexts)\n",
        "\n",
        "# ---- Curves: running average reward & cumulative regret ---------------------\n",
        "avg_rand = np.cumsum(rewards_rand) / np.arange(1, T+1)\n",
        "avg_eps  = np.cumsum(rewards_eps)  / np.arange(1, T+1)\n",
        "avg_ucb  = np.cumsum(rewards_ucb)  / np.arange(1, T+1)\n",
        "\n",
        "cumreg_rand = np.cumsum(regret_rand)\n",
        "cumreg_eps  = np.cumsum(regret_eps)\n",
        "cumreg_ucb  = np.cumsum(regret_ucb)\n",
        "\n",
        "# ---- Plot -------------------------------------------------------------------\n",
        "plt.figure(figsize=(12,4.5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(avg_rand, label=\"Random\")\n",
        "plt.plot(avg_eps,  label=f\"Contextual ε-Greedy (ε={epsilon})\")\n",
        "plt.plot(avg_ucb,  label=f\"LinUCB (α={alpha}, λ={lambda_})\")\n",
        "plt.title(\"Average Reward (running mean)\")\n",
        "plt.xlabel(\"Time step\"); plt.ylabel(\"Average reward\"); plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(cumreg_rand, label=\"Random\")\n",
        "plt.plot(cumreg_eps,  label=f\"Contextual ε-Greedy (ε={epsilon})\")\n",
        "plt.plot(cumreg_ucb,  label=f\"LinUCB (α={alpha}, λ={lambda_})\")\n",
        "plt.title(\"Cumulative Regret\")\n",
        "plt.xlabel(\"Time step\"); plt.ylabel(\"Cumulative regret\"); plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- Quick totals ------------------------------------------------------------\n",
        "print(\"Totals over T steps\")\n",
        "print(f\"  Random:              reward={rewards_rand.sum():5d}, final avg={avg_rand[-1]:.3f}, cum regret={cumreg_rand[-1]:.1f}\")\n",
        "print(f\"  Contextual ε-Greedy: reward={rewards_eps.sum():5d}, final avg={avg_eps[-1]:.3f}, cum regret={cumreg_eps[-1]:.1f}\")\n",
        "print(f\"  LinUCB:              reward={rewards_ucb.sum():5d}, final avg={avg_ucb[-1]:.3f}, cum regret={cumreg_ucb[-1]:.1f}\")\n"
      ],
      "metadata": {
        "id": "ZnQsBrIqb-rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Before vs After Learning from 10,000 Samples ====================\n",
        "import numpy as np\n",
        "\n",
        "# --- Helpers (pretty printing + per-step explanation) -------------------------\n",
        "def _fmt_kv(names, values):\n",
        "    return \", \".join(f\"{n}={v:.2f}\" for n, v in zip(names, values))\n",
        "\n",
        "def _explain_step(env, policy, x, label, show_reason=True):\n",
        "    arm_names = env.arm_names\n",
        "    k = env.k\n",
        "\n",
        "    # Reasoning for each policy (before selecting)\n",
        "    reasoning = \"\"\n",
        "    if show_reason:\n",
        "        if isinstance(policy, ContextualEpsGreedy):\n",
        "            scores = policy.Q @ x\n",
        "            max_val = np.max(scores)\n",
        "            best = np.flatnonzero(scores == max_val)\n",
        "            reasoning = \"scores: [\" + \", \".join(f\"{arm_names[a]}={scores[a]:.3f}\" for a in range(k)) + \"]\"\n",
        "            if len(best) > 1:\n",
        "                reasoning += f\" | tie among: {', '.join(arm_names[a] for a in best)}\"\n",
        "        elif isinstance(policy, LinUCB):\n",
        "            parts = []\n",
        "            for a in range(k):\n",
        "                A_inv_x = np.linalg.solve(policy.A[a], x)\n",
        "                theta_hat = np.linalg.solve(policy.A[a], policy.b[a])\n",
        "                pred = float(theta_hat @ x)\n",
        "                bonus = float(policy.alpha * np.sqrt(x @ A_inv_x))\n",
        "                parts.append(f\"{arm_names[a]}={pred:.3f}+{bonus:.3f}={pred+bonus:.3f}\")\n",
        "            reasoning = \"pred+bonus=UCB: [\" + \", \".join(parts) + \"]\"\n",
        "        else:\n",
        "            reasoning = \"Random policy (no scoring).\"\n",
        "\n",
        "    # Select, observe, update\n",
        "    arm = policy.select(x)\n",
        "    r, p = env.click(arm, x)\n",
        "    policy.update(arm, x, r)\n",
        "\n",
        "    # Bayes-optimal (for teaching/inspection only)\n",
        "    true_scores = env.theta @ x\n",
        "    best_true = int(np.argmax(true_scores))\n",
        "    p_best = 1.0 / (1.0 + np.exp(-true_scores[best_true]))\n",
        "\n",
        "    print(f\"[{label:<10}] picked={env.arm_names[arm]:10s} | p(click)={p:.2f} | \"\n",
        "          f\"clicked? {'YES' if r==1 else 'no '} | true best={env.arm_names[best_true]} (p*={p_best:.2f})\")\n",
        "    if show_reason and reasoning:\n",
        "        print(f\"            {reasoning}\")\n",
        "\n",
        "def walkthrough_before_after_with_random(\n",
        "    pre_steps=6, train_T=10000, post_steps=6,\n",
        "    epsilon=0.1, alpha=1.0, lambda_=1.0, seed=7\n",
        "):\n",
        "    # 1) Fixed contexts for fair comparison (same for all policies & phases)\n",
        "    ctx_gen = ContextualNewsEnv(seed=seed)\n",
        "    pre_contexts   = [ctx_gen.sample_context() for _ in range(pre_steps)]\n",
        "    train_contexts = [ctx_gen.sample_context() for _ in range(train_T)]\n",
        "    post_contexts  = [ctx_gen.sample_context() for _ in range(post_steps)]\n",
        "\n",
        "    # 2) Independent envs (same θ, since same seed) for each policy & phase\n",
        "    env_rand_pre   = ContextualNewsEnv(seed=seed)\n",
        "    env_eps_pre    = ContextualNewsEnv(seed=seed)\n",
        "    env_ucb_pre    = ContextualNewsEnv(seed=seed)\n",
        "\n",
        "    env_rand_post  = ContextualNewsEnv(seed=seed)\n",
        "    env_eps_train  = ContextualNewsEnv(seed=seed)\n",
        "    env_ucb_train  = ContextualNewsEnv(seed=seed)\n",
        "    env_eps_post   = ContextualNewsEnv(seed=seed)\n",
        "    env_ucb_post   = ContextualNewsEnv(seed=seed)\n",
        "\n",
        "    # 3) Policies (Random has no learning; ε-Greedy & LinUCB start from scratch)\n",
        "    rand_pre  = RandomPolicy(k=env_rand_pre.k, seed=seed)\n",
        "    eps       = ContextualEpsGreedy(k=env_eps_pre.k, d=env_eps_pre.d, epsilon=epsilon, seed=seed)\n",
        "    ucb       = LinUCB(k=env_ucb_pre.k, d=env_ucb_pre.d, alpha=alpha, lambda_=lambda_)\n",
        "\n",
        "    # --- BEFORE LEARNING: first N steps --------------------------------------\n",
        "    print(\"=== BEFORE LEARNING (first 6 steps) ===\")\n",
        "    for t, x in enumerate(pre_contexts, 1):\n",
        "        print(f\"\\nStep {t} context: {_fmt_kv(env_eps_pre.feature_names, x)}\")\n",
        "        _explain_step(env_rand_pre, rand_pre, x, label=\"Random\")\n",
        "        _explain_step(env_eps_pre,  eps,     x, label=\"ε-Greedy\")\n",
        "        _explain_step(env_ucb_pre,  ucb,     x, label=\"LinUCB\")\n",
        "\n",
        "    # --- TRAIN: 10,000 online rounds for ε-Greedy and LinUCB -----------------\n",
        "    for x in train_contexts:\n",
        "        # ε-Greedy training\n",
        "        arm = eps.select(x)\n",
        "        r, _ = env_eps_train.click(arm, x)\n",
        "        eps.update(arm, x, r)\n",
        "        # LinUCB training\n",
        "        arm = ucb.select(x)\n",
        "        r, _ = env_ucb_train.click(arm, x)\n",
        "        ucb.update(arm, x, r)\n",
        "\n",
        "    # Fresh Random for post phase (still stateless, no training)\n",
        "    rand_post = RandomPolicy(k=env_rand_post.k, seed=seed)\n",
        "\n",
        "    # --- AFTER LEARNING FROM 10,000 SAMPLES: next N steps --------------------\n",
        "    print(\"\\n=== AFTER LEARNING FROM 10,000 SAMPLES (next 6 steps) ===\")\n",
        "    for t, x in enumerate(post_contexts, 1):\n",
        "        print(f\"\\nStep {t} context: {_fmt_kv(env_eps_post.feature_names, x)}\")\n",
        "        _explain_step(env_rand_post, rand_post, x, label=\"Random\")\n",
        "        _explain_step(env_eps_post,  eps,       x, label=\"ε-Greedy\")\n",
        "        _explain_step(env_ucb_post,  ucb,       x, label=\"LinUCB\")\n",
        "\n",
        "# Run the demonstration\n",
        "walkthrough_before_after_with_random(\n",
        "    pre_steps=6, train_T=10000, post_steps=6,\n",
        "    epsilon=0.1, alpha=1.0, lambda_=1.0, seed=7\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Btawey8Dc4da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xpk4QSqxb7Dj"
      }
    }
  ]
}