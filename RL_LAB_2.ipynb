{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "You need to run this before doing the tasks:"
      ],
      "metadata": {
        "id": "RF8HGKO1HI_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-P-rrD7DWuD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class GridSSP:\n",
        "    def __init__(self, size=5, goal=(4, 4), gamma=1.0,seed=0):\n",
        "        self.size = size\n",
        "        self.goal = goal\n",
        "        self.gamma = gamma\n",
        "        self.actions = ['U', 'D', 'L', 'R']\n",
        "        self.states = [(i, j) for i in range(size) for j in range(size)]\n",
        "        self.state_idx = {s: idx for idx, s in enumerate(self.states)}\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.custom_probs = self._generate_probs()\n",
        "\n",
        "\n",
        "    def _generate_probs(self):\n",
        "        # Input:  None\n",
        "        # Output: dict mapping (state, action) -> {'intended','slip1','slip2','stay'} probabilities\n",
        "        #np.random.seed(0)\n",
        "        probs = {}\n",
        "        for s in self.states:\n",
        "            for a in self.actions:\n",
        "                p1 = self.rng.uniform(0.5, 0.8)\n",
        "                p2 = self.rng.uniform(0.05, 0.10)\n",
        "                p3 = self.rng.uniform(0.05, 0.10)\n",
        "                stay = max(0.0, 1.0 - p1 - p2 - p3)\n",
        "                probs[(s, a)] = {'intended': p1, 'slip1': p2, 'slip2': p3, 'stay': stay}\n",
        "        return probs\n",
        "\n",
        "    def transition_probs(self, s, a):\n",
        "        # Input:  s (tuple[int,int]) state, a (str in {'U','D','L','R'}) action\n",
        "        # Output: list of (next_state (tuple[int,int]), probability (float)) pairs; sums to 1\n",
        "        # Note:   goal is absorbing for any action\n",
        "        if s == self.goal:\n",
        "            return [(self.goal, 1.0)]\n",
        "\n",
        "        i, j = s\n",
        "        moves = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
        "        sideways = {'U': ['L', 'R'], 'D': ['L', 'R'], 'L': ['U', 'D'], 'R': ['U', 'D']}\n",
        "        probs = []\n",
        "        pset = self.custom_probs[(s, a)]\n",
        "        ni, nj = max(0, min(i + moves[a][0], self.size - 1)), max(0, min(j + moves[a][1], self.size - 1))\n",
        "        probs.append(((ni, nj), pset['intended']))\n",
        "        for idx, side in enumerate(sideways[a]):\n",
        "            si, sj = i + moves[side][0], j + moves[side][1]\n",
        "            si, sj = max(0, min(si, self.size - 1)), max(0, min(sj, self.size - 1))\n",
        "            probs.append(((si, sj), pset['slip1'] if idx == 0 else pset['slip2']))\n",
        "        probs.append(((i, j), pset['stay']))\n",
        "        return probs\n",
        "\n",
        "    def reward(self, s, a, s_next):\n",
        "        # Input: s (state), a (action), s_next (next state)\n",
        "        # Output: scalar reward (float)\n",
        "        # Default: -1 per step until the absorbing goal (0 there)\n",
        "        return 0.0 if s_next == self.goal else -1.0\n",
        "\n",
        "    def evaluate_policy(self, policy):\n",
        "        # Input:  policy (dict) mapping state -> action ('U','D','L','R')\n",
        "        # Output: V (np.ndarray shape (size, size)) value function with gamma=1 and V(goal)=0\n",
        "        # Method: build full P,R; remove goal row/col; solve (I - P_T) v_T = R_T\n",
        "\n",
        "        n = self.size * self.size\n",
        "        P = np.zeros((n, n), dtype=float)\n",
        "        R = np.zeros(n, dtype=float)\n",
        "\n",
        "        for s in self.states:\n",
        "            idx = self.state_idx[s]\n",
        "            if s == self.goal:\n",
        "                P[idx, idx] = 1.0  # absorbing\n",
        "                R[idx] = 0.0\n",
        "                continue\n",
        "\n",
        "            a = policy[s]\n",
        "\n",
        "            # Aggregate duplicates from transition_probs(s,a)\n",
        "            row = {}\n",
        "            for s2, p in self.transition_probs(s, a):\n",
        "                jdx = self.state_idx[s2]\n",
        "                row[jdx] = row.get(jdx, 0.0) + float(p)\n",
        "\n",
        "            # Assert that the transition probabilities sum to one\n",
        "            total = sum(row.values())\n",
        "            assert abs(total - 1.0) < 1e-12, f\"P row sum={total} at state {s}, action {a}\"\n",
        "\n",
        "            # Fill P and immediate reward (-1 per step -> expected is -1)\n",
        "            for jdx, p in row.items():\n",
        "                P[idx, jdx] += p\n",
        "            R[idx] = -1.0\n",
        "\n",
        "        # Remove goal row/col -> transient subsystem\n",
        "        g = self.state_idx[self.goal]\n",
        "        mask = np.ones(n, dtype=bool)\n",
        "        mask[g] = False\n",
        "        P_T = P[mask][:, mask]\n",
        "        R_T = R[mask]\n",
        "\n",
        "        # Solve (I - P_T) v_T = R_T  (gamma = 1)\n",
        "        v_T = np.linalg.solve(np.eye(P_T.shape[0]) - P_T, R_T)\n",
        "\n",
        "        # Stitch back with V(goal)=0\n",
        "        V = np.zeros(n, dtype=float)\n",
        "        V[mask] = v_T\n",
        "        V[g] = 0.0\n",
        "\n",
        "        return V.reshape((self.size, self.size))\n",
        "\n",
        "\n",
        "    def value_iteration(self, tol=1e-8, max_iters=10000):\n",
        "        # Input:  tol (float) stopping threshold on the max Bellman residual;\n",
        "        #         max_iters (int) cap on iterations\n",
        "        # Output: (V_grid, policy, info) where\n",
        "        #         V_grid is np.ndarray shape (size,size),\n",
        "        #         policy is dict[state]->best action ('U','D','L','R') with goal -> '.',\n",
        "        #         info is dict with {'iterations': int, 'max_residual': float}\n",
        "        import numpy as np\n",
        "\n",
        "        gamma = float(self.gamma)\n",
        "\n",
        "        # Initialize V(s)=0; enforce V(goal)=0 at all times\n",
        "        V = {s: 0.0 for s in self.states}\n",
        "        V[self.goal] = 0.0\n",
        "\n",
        "        def q_value(s, a, Vsnap):\n",
        "            \"\"\"One-step lookahead: Q(s,a) = sum_{s'} P(s'|s,a) [ R(s,a,s') + gamma * V(s') ]\"\"\"\n",
        "            val = 0.0\n",
        "            for s2, p in self.transition_probs(s, a):\n",
        "                r = self.reward(s, a, s2)\n",
        "                # with gamma=1, V(goal) is fixed at 0; with gamma<1, this line still works\n",
        "                val += p * (r + gamma * Vsnap[s2])\n",
        "            return val\n",
        "\n",
        "        # Value iteration loop\n",
        "        it = 0\n",
        "        for it in range(1, max_iters + 1):\n",
        "            V_old = V.copy()\n",
        "            delta = 0.0\n",
        "            for s in self.states:\n",
        "                if s == self.goal:\n",
        "                    V[s] = 0.0\n",
        "                    continue\n",
        "                # Bellman optimality backup\n",
        "                best = -np.inf\n",
        "                for a in self.actions:\n",
        "                    qa = q_value(s, a, V_old)\n",
        "                    if qa > best:\n",
        "                        best = qa\n",
        "                V[s] = best\n",
        "                delta = max(delta, abs(V[s] - V_old[s]))\n",
        "            # Convergence check\n",
        "            if delta < tol:\n",
        "                break\n",
        "\n",
        "        # Greedy policy extraction w.r.t. the final V\n",
        "        policy = {}\n",
        "        for s in self.states:\n",
        "            if s == self.goal:\n",
        "                policy[s] = '.'\n",
        "                continue\n",
        "            best_a, best_val = None, -np.inf\n",
        "            for a in self.actions:\n",
        "                qa = q_value(s, a, V)\n",
        "                if qa > best_val:\n",
        "                    best_val, best_a = qa, a\n",
        "            policy[s] = best_a\n",
        "\n",
        "        # Pack V into a grid\n",
        "        V_grid = np.zeros((self.size, self.size), dtype=float)\n",
        "        for (i, j), val in V.items():\n",
        "            V_grid[i, j] = val\n",
        "\n",
        "        info = {'iterations': it, 'max_residual': float(delta)}\n",
        "        return V_grid, policy, info\n",
        "\n",
        "\n",
        "    ####\n",
        "    #### Here comes the part used to create a simulation\n",
        "    ####\n",
        "\n",
        "    def _sample_from_dist(self, items, probs, rng):\n",
        "        # Input:  items (list[Any]), probs (list[float]) summing to 1, rng (np.random.Generator)\n",
        "        # Output: one sampled item from items according to probs\n",
        "        c = np.cumsum(probs)\n",
        "        r = rng.random()\n",
        "        idx = int(np.searchsorted(c, r, side=\"right\"))\n",
        "        return items[idx]\n",
        "\n",
        "    def _pick_action(self, policy, s, rng):\n",
        "        # Input:  policy (dict[state]->action OR dict[state]->{action:prob}), s (tuple), rng (np.random.Generator)\n",
        "        # Output: action (str in {'U','D','L','R'})\n",
        "        pi_s = policy[s]\n",
        "        if isinstance(pi_s, str):\n",
        "            return pi_s\n",
        "        acts, probs = zip(*pi_s.items())\n",
        "        assert abs(sum(probs) - 1.0) < 1e-12, f\"Policy probs at {s} must sum to 1.\"\n",
        "        return self._sample_from_dist(list(acts), list(probs), rng)\n",
        "\n",
        "    def step(self, s, a, rng=None):\n",
        "        # Input:  s (state tuple), a (action str), rng (np.random.Generator or None)\n",
        "        # Output: (s_next (state tuple), r (float), done (bool))\n",
        "        if s == self.goal:\n",
        "            return self.goal, 0.0, True\n",
        "        if rng is None:\n",
        "            rng = np.random.default_rng()\n",
        "\n",
        "        outcomes = self.transition_probs(s, a)  # list[(s2, p)] whose probs sum to 1\n",
        "        states, probs = zip(*outcomes)\n",
        "        s_next = self._sample_from_dist(list(states), list(probs), rng)\n",
        "        r = self.reward(s, a, s_next)  # uses the environment's reward\n",
        "        done = (s_next == self.goal)\n",
        "        return s_next, r, done\n",
        "\n",
        "    def simulate(self, policy, start, max_steps=1000, rng=None):\n",
        "        # Input:  policy (deterministic or stochastic dict), start (state tuple),\n",
        "        #         max_steps (int), rng (np.random.Generator or None)\n",
        "        # Output: dict with {'states': [...], 'actions': [...],\n",
        "        #                    'total_reward': float, 'steps': int, 'reached_goal': bool}\n",
        "        if rng is None:\n",
        "            rng = np.random.default_rng()\n",
        "\n",
        "        s = start\n",
        "        states = [s]\n",
        "        actions = []\n",
        "        total_reward = 0.0\n",
        "\n",
        "        if s == self.goal:\n",
        "            return {'states': states, 'actions': actions,\n",
        "                    'total_reward': 0.0, 'steps': 0, 'reached_goal': True}\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            a = self._pick_action(policy, s, rng)\n",
        "            s_next, r, done = self.step(s, a, rng=rng)\n",
        "            actions.append(a)\n",
        "            states.append(s_next)\n",
        "            total_reward += r\n",
        "            s = s_next\n",
        "            if done:\n",
        "                return {'states': states, 'actions': actions,\n",
        "                        'total_reward': total_reward, 'steps': t + 1, 'reached_goal': True}\n",
        "\n",
        "        return {'states': states, 'actions': actions,\n",
        "                'total_reward': total_reward, 'steps': max_steps, 'reached_goal': False}\n",
        "\n",
        "\n",
        "def build_policy(states, fixed_action):\n",
        "    # Input:  states (iterable of (i,j)), fixed_action (str in {'U','D','L','R'})\n",
        "    # Output: dict mapping each state -> fixed_action\n",
        "    return {s: fixed_action for s in states}\n",
        "\n",
        "\n",
        "def plot_value_function(V, title):\n",
        "    # Input:  V (np.ndarray of shape (H, W)) value grid; title (str) figure title\n",
        "    # Output: None (displays a heatmap with numeric values via matplotlib)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(V, cmap='YlGn', origin='upper')\n",
        "    for i in range(V.shape[0]):\n",
        "        for j in range(V.shape[1]):\n",
        "            plt.text(j, i, f\"{V[i, j]:.1f}\", ha='center', va='center', color='black')\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_many_simulations(env, policy, n_sims=1000, start=(0,0), max_steps=100000):\n",
        "    # Input:  env (GridSSP), policy (deterministic or stochastic dict),\n",
        "    #         n_sims (int), start (tuple), max_steps (int)\n",
        "    # Output: dict with arrays and summary stats (means)\n",
        "    total_rewards = np.zeros(n_sims, dtype=float)\n",
        "    steps = np.zeros(n_sims, dtype=int)\n",
        "    reached = np.zeros(n_sims, dtype=bool)\n",
        "\n",
        "    for k in range(n_sims):\n",
        "        rng = np.random.default_rng(k)  # different seed each run\n",
        "        out = env.simulate(policy, start=start, max_steps=max_steps, rng=rng)\n",
        "        total_rewards[k] = out['total_reward']\n",
        "        steps[k] = out['steps']\n",
        "        reached[k] = out['reached_goal']\n",
        "\n",
        "    summary = {\n",
        "        'mean_total_reward': float(np.mean(total_rewards)),\n",
        "        'mean_steps': float(np.mean(steps)),\n",
        "        'success_rate': float(np.mean(reached)),\n",
        "    }\n",
        "    return {\n",
        "        'total_rewards': total_rewards,\n",
        "        'steps': steps,\n",
        "        'reached_goal': reached,\n",
        "        'summary': summary,\n",
        "    }\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "def animate_simulation(env, sim_output, interval=250, trail=True):\n",
        "    # Input: env (GridSSP), sim_output (dict from env.simulate), interval (ms), trail (bool)\n",
        "    # Output: Matplotlib animation object\n",
        "    states = sim_output['states']  # [(i,j), ...]\n",
        "    H = W = env.size\n",
        "    gi, gj = env.goal\n",
        "    T = len(states)  # number of frames\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.set_xlim(-0.5, W-0.5)\n",
        "    ax.set_ylim(-0.5, H-0.5)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()  # top-left is (0,0)\n",
        "\n",
        "    # grid lines\n",
        "    for x in range(W):\n",
        "        ax.axvline(x-0.5, color='lightgray', linewidth=0.8)\n",
        "    for y in range(H):\n",
        "        ax.axhline(y-0.5, color='lightgray', linewidth=0.8)\n",
        "\n",
        "    # goal highlight\n",
        "    ax.add_patch(plt.Rectangle((gj-0.5, gi-0.5), 1, 1, fill=True, alpha=0.25, color='green'))\n",
        "    ax.set_title(\"Simulation animation\")\n",
        "\n",
        "    # moving dot + optional trail\n",
        "    dot, = ax.plot([], [], 'o', markersize=10)\n",
        "    path_line = None\n",
        "    if trail:\n",
        "        (path_line,) = ax.plot([], [], '-', linewidth=2, alpha=0.6)\n",
        "\n",
        "    # time index text (top-left in axes coords)\n",
        "    time_text = ax.text(0.02, 0.06, \"\", transform=ax.transAxes, fontsize=12, ha='left', va='bottom',\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.7))\n",
        "\n",
        "    def init():\n",
        "        dot.set_data([], [])\n",
        "        time_text.set_text(\"\")\n",
        "        if path_line is not None:\n",
        "            path_line.set_data([], [])\n",
        "            return dot, time_text, path_line\n",
        "        return dot, time_text\n",
        "\n",
        "    def to_xy(state):\n",
        "        i, j = state\n",
        "        return j, i  # x=j, y=i\n",
        "\n",
        "    xs, ys = zip(*[to_xy(s) for s in states])  # sequences\n",
        "\n",
        "    def update(frame):\n",
        "        # dot needs sequences, not scalars\n",
        "        dot.set_data([xs[frame]], [ys[frame]])\n",
        "        time_text.set_text(f\"t = {frame} / {T-1}\")\n",
        "        if path_line is not None:\n",
        "            path_line.set_data(xs[:frame+1], ys[:frame+1])\n",
        "            return dot, time_text, path_line\n",
        "        return dot, time_text\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update, init_func=init, frames=T, interval=interval, blit=True\n",
        "    )\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "\n",
        "# We create the MDP here, and dont change it through your expriements\n",
        "# You can change the seed to create different instances of the MPD\n",
        "env = GridSSP(seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:** Identify the MDP\n",
        "\n",
        "Use the provided code to create an instance of the wireless routing environment on\n",
        "the\n",
        "\n",
        "**(a) Explore the transitions.**\n",
        "For a given state and action, the transition probabilities define how likely the packet is to move to each possible next state.  Plot these probabilities for a specific state–action pair. For example, examine state $(2,2)$ with action 'U'. Try other combinations (e.g., $(0,3)$ with 'R') and compare.\n",
        "\n",
        "**(b) Interpret the results.**\n",
        "Be prepared to explain in your own words what the plotted transition probabilities mean in this context:\n",
        "- Which next states are most likely?\n",
        "- What does a high probability of “staying in place” indicate?\n",
        "- How do these transitions relate to network reliability or congestion?"
      ],
      "metadata": {
        "id": "jBKUXw5hJxdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Task 1\n",
        "# This shows the transition probilities P(s'|s,a) when the state is s=(2,2) and the action is 'U'\n",
        "\n",
        "\n",
        "#########################################\n",
        "#### This part you can play with ########\n",
        "#########################################\n",
        "\n",
        "state = (2, 2) # Test changing the state\n",
        "action = 'U'   # Test changing the actions, possible actions are 'U', 'D', 'L', 'R'\n",
        "\n",
        "#########################################\n",
        "#########################################\n",
        "#########################################\n",
        "\n",
        "probs = env.transition_probs(state, action)\n",
        "for s2, p in probs:\n",
        "    print(f\"{state} --'{action}'--> {s2} with prob {p:.2f}\")\n",
        "\n",
        "# Make bar plot:\n",
        "labels = [str(s) for s, _ in probs]\n",
        "values = [p for _, p in probs]\n",
        "plt.bar(labels, values)\n",
        "plt.title(f\"Transitions from {state} using '{action}'\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dx8s6KlaD0EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Evaluate Fixed Policies via Linear System\n",
        "\n",
        "\n",
        "We provide code that computes the value function for two fixed policies:\n",
        "\n",
        "**Policy 1:** Always try to send the packet to the 'R' (right).\n",
        "\n",
        "**Policy 2:** Always try to send the packet to the 'D' (down).\n",
        "\n",
        "\n",
        "Run both examples and inspect the provided heatmaps. Be prepared to answer the Task 2 questions in the handout"
      ],
      "metadata": {
        "id": "Pv9-rV5UTvV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Task 2\n",
        "### Here we plot the value function for the policy that always attempts to go right\n",
        "#\n",
        "\n",
        "\n",
        "# Create the policy:\n",
        "pi_right = build_policy(env.states, 'R')\n",
        "\n",
        "# Perform policy evalution by solivng a linear system:\n",
        "V_right = env.evaluate_policy(pi_right)\n",
        "\n",
        "# Plot value function heatmap\n",
        "plot_value_function(V_right, \"Value Function: Always Go Right\")\n"
      ],
      "metadata": {
        "id": "W2WGPPAGBya6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task 2 (Part 2)\n",
        "### Here we plot the value function for the policy that always attempts to go down\n",
        "#\n",
        "\n",
        "# Create the policy:\n",
        "pi_down = build_policy(env.states, 'D')\n",
        "\n",
        "# Perform policy evalution by solivng a linear system:\n",
        "V_up = env.evaluate_policy(pi_down)\n",
        "\n",
        "# Plot value function heatmap:\n",
        "plot_value_function(V_up, \"Value Function: Always Go Down\")"
      ],
      "metadata": {
        "id": "d6DU0YyQjEjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Simulating Policies & Estimating Value\n",
        "\n",
        "**What to do**\n",
        "1. **Play with simulations:** Use the code given in Part 1 and Part 2 below. It uses simulates trajectories for different policies (e.g., always `R`, always `D`, or a mix like 70% `R` / 30% `D`). You can visualize a few trajectories (this is illustrated in Part 3).\n",
        "\n",
        "2. **Main experiment** Use the code provided in Part 4. Run many episodes from start state `(0,0)` for **Policy 1** (always `R`) and **Policy 2** (always `D`). Record:\n",
        "   - average return (mean over episodes),\n",
        "   - mean number of steps,\n",
        "   - success rate (fraction reaching the goal).\n",
        "3. **Compare to Task 2:** Compare each policy’s **empirical average return** to the **value** you computed via the linear-system evaluation in Task 2 for the same start state.\n",
        "4. **Be ready to explain:**\n",
        "   - What is a trajectory? Why is it (typically) different at each episode? What controls\n",
        "the randomness?\n",
        "   - How the empirical average return **estimates** \\(V^\\pi(s_0)\\): this is **Monte Carlo policy evaluation** (no model used—only observed states and rewards). With enough episodes, the average return converges to the true value (Lecture 5).\n",
        "\n"
      ],
      "metadata": {
        "id": "ImMnNxh0SxB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 (Part 1)\n",
        "#\n",
        "# This simulates one instance of the MDP where we follow Policy 1 (always try\n",
        "# to send the package to right) and we start in state (0,0)\n",
        "#\n",
        "#\n",
        "\n",
        "# This is the random seed, change it to try different realizations\n",
        "simulation_seed = 100\n",
        "\n",
        "# Reproducible RNG\n",
        "rng = np.random.default_rng(simulation_seed)\n",
        "\n",
        "# Create the policy, always try right\n",
        "pi_right = {s: 'R' for s in env.states}\n",
        "\n",
        "# Simulate:\n",
        "out1 = env.simulate(pi_right, start=(0, 0), max_steps=5000, rng=rng)\n",
        "\n",
        "# Print the results:\n",
        "print(\"Deterministic (Right):\", out1['reached_goal'], out1['steps'], out1['total_reward'])\n",
        "print(\"Trajectory (first 100 states):\", out1['states'][:100])\n",
        "\n"
      ],
      "metadata": {
        "id": "JCKEG2TFyfff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 (Part 2)\n",
        "#\n",
        "# This simulates one instance of the MDP where we follow a random policy that\n",
        "# goes Right 'R' with probability 0.7 and Down'D' with probablity 0.3 and start\n",
        "# in state (0,0)\n",
        "#\n",
        "\n",
        "# This is the random seed, change it to try different realizations\n",
        "simulation_seed = 100\n",
        "rng = np.random.default_rng(simulation_seed)\n",
        "\n",
        "# Stochastic policy: 70% Right, 30% Down\n",
        "pi_mix = {s: {'R': 0.7, 'D': 0.3} for s in env.states}\n",
        "\n",
        "# Simulate a stochastic policy from (2,2)\n",
        "out2 = env.simulate(pi_mix, start=(2, 2), max_steps=500, rng=rng)\n",
        "print(\"Stochastic (0.7R/0.3D):\", out2['reached_goal'], out2['steps'], out2['total_reward'])\n",
        "print(\"Trajectory (first 100 states):\", out2['states'][:10])\n"
      ],
      "metadata": {
        "id": "aWm6Y3iA1mS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 (Part 3)\n",
        "#\n",
        "# This is optional.\n",
        "#\n",
        "# If you run this you can visuallize one simulation. Play around with changing\n",
        "# the policy, changing the random seed, the start state.\n",
        "\n",
        "\n",
        "# Take any policy\n",
        "pi_right = {s: 'R' for s in env.states}\n",
        "#pi_mix = {s: {'R': 0.7, 'D': 0.3} for s in env.states}\n",
        "\n",
        "\n",
        "# This is the random seed, change it to try different realizations\n",
        "simulation_seed = 100\n",
        "rng = np.random.default_rng(simulation_seed)\n",
        "\n",
        "# Run one simulation from (0,0)\n",
        "sim_out = env.simulate(pi_right, start=(0,0), max_steps=1000, rng=rng)\n",
        "\n",
        "# Animate (in Jupyter, this will display below)\n",
        "anim = animate_simulation(env, sim_out, interval=200, trail=True)\n",
        "from IPython.display import HTML\n",
        "HTML(anim.to_jshtml())  # or display(anim)"
      ],
      "metadata": {
        "id": "ZxRzy_d568hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 (Part 4): Monte Carlo estimation of value under a fixed policy\n",
        "#\n",
        "# In this section we estimate the value of a policy by running many simulated\n",
        "# episodes and averaging the observed returns — i.e., a Monte Carlo (MC)\n",
        "# estimator. You can change:\n",
        "#   - the policy (e.g., always 'R' vs always 'D', or a stochastic policy),\n",
        "#   - the start state,\n",
        "#   - the number of simulations (n_sims).\n",
        "#\n",
        "# What to do:\n",
        "# 1) Run multiple simulations following a fixed policy from s0 = (0,0).\n",
        "# 2) Compute the empirical average return across runs.\n",
        "# 3) Compare this average return for Policy 1 and Policy 2 to the value you\n",
        "#    computed earlier via the linear-system method (policy evaluation).\n",
        "#\n",
        "# Notes:\n",
        "# • This uses no model knowledge beyond observing states and rewards — it is an\n",
        "#   example of a Reinforcement Learning approach (Monte Carlo policy evaluation),\n",
        "#   which we will discuss in Lecture 5.\n",
        "# • With enough runs, the MC average should approach the true value V^π(s0).\n",
        "\n",
        "pi_right = {s: 'R' for s in env.states}\n",
        "\n",
        "results = run_many_simulations(env, pi_right, n_sims=100, start=(0,0), max_steps=100000)\n",
        "print(\"Summary:\", results['summary'])"
      ],
      "metadata": {
        "id": "PFRcP-xh7Fwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Value Iteration (Optimal Value & Policy)\n",
        "\n",
        "**Your tasks (no solutions here):**\n",
        "- **Explain “optimal”:** Briefly state, in your own words, what it means for a policy/value to be *optimal* in this setting.\n",
        "- **Explain the idea of value iteration:** Summarize how repeated Bellman *optimality* backups produce the fixed point used here; mention why we keep \\(V(\\text{goal})=0\\) and when we stop iterating.\n",
        "- **From \\(V^*\\) to \\(\\pi^*\\):** Describe how one-step lookahead with \\(V^*\\) yields an optimal policy (argmax over actions).\n",
        "- **Inspect results:** Look at the optimal value heatmap and (optionally) visualize arrows/actions for \\(\\pi^*\\). Relate patterns to the goal location and slip dynamics.\n",
        "\n",
        "**Optional prompts:**\n",
        "- Compare \\(V^*(s_0)\\) with values from Task 2’s fixed policies at the same start state \\(s_0\\).\n",
        "- If you simulate trajectories under \\(\\pi^*\\), what do you observe about returns and paths?\n",
        "\n"
      ],
      "metadata": {
        "id": "uPlkpo5GhwSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_opt, pi_opt, info = env.value_iteration(tol=1e-8, max_iters=10000)\n",
        "print(\"VI iterations:\", info['iterations'], \"max residual:\", info['max_residual'])\n",
        "plot_value_function(V_opt, \"Optimal Value (Value Iteration)\")"
      ],
      "metadata": {
        "id": "1jA1bRDXh354"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yiiH-nvvh6zZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}